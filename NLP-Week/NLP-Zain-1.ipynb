{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzAhD4Py6rqf",
    "outputId": "98718da0-6ccf-4f60-dc54-900760b9e12f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4jk41KR8B4u"
   },
   "source": [
    "**Stemming using NLTK:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0TRaZjs6roD",
    "outputId": "1033273d-7635-42c9-9183-06c0ad430548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'jump', 'friendli']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample words to stem\n",
    "words = [\"running\", \"flies\", \"jumping\", \"friendly\"]\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem the words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print the stemmed words\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jco_DH8-8PKs"
   },
   "source": [
    "**Lemmatization using spaCy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CxQ-IxB6rlh",
    "outputId": "e0f20c9e-03a7-47fc-bf0b-64bd74add5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fly', 'jump', 'friendly']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample words to lemmatize\n",
    "words = [\"running\", \"flies\", \"jumping\", \"friendly\"]\n",
    "\n",
    "# Lemmatize the words\n",
    "lemmatized_words = [token.lemma_ for token in nlp(\" \".join(words))]\n",
    "\n",
    "# Print the lemmatized words\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZJNKklHEzaT"
   },
   "source": [
    "**Text Normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbEQWS1XD4N2",
    "outputId": "cc1d0b99-b7b4-44dc-97ef-c263653a7cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love metapi. please great job! ai is amazing.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with variations\n",
    "text = \"I luv metapi. Plz gr8 job! AI is amazin'.\"\n",
    "\n",
    "# Define a simple text normalization function\n",
    "def text_normalization(text):\n",
    "    # Replace abbreviations and acronyms\n",
    "    text = text.replace(\"luv\", \"love\")\n",
    "    text = text.replace(\"Plz\", \"Please\")\n",
    "    text = text.replace(\"gr8\", \"great\")\n",
    "    text = text.replace(\"amazin'\", \"amazing\")\n",
    "\n",
    "    # Convert to lowercase (optional)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply text normalization\n",
    "normalized_text = text_normalization(text)\n",
    "\n",
    "# Print the normalized text\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnjkqUXXH0b9"
   },
   "source": [
    "**Removing Redundancy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmYql1rID4LW",
    "outputId": "1e2e03cd-4e35-47cd-a000-8378e347a063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence.\n",
      "Another example sentence.\n",
      "Some unique text here.\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "# Sample list of text entries with potential duplicates\n",
    "text_entries = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"This is an example sentence.\",  # Duplicate\n",
    "    \"Yet another example sentence.\",\n",
    "    \"Some unique text here.\",\n",
    "    \"This is an example sentence.\"   # Duplicate\n",
    "]\n",
    "\n",
    "# Function to remove duplicates based on similarity threshold\n",
    "def remove_redundancy(text_entries, similarity_threshold=0.8):\n",
    "    # Initialize a list to store the deduplicated entries\n",
    "    deduplicated_entries = []\n",
    "\n",
    "    # Iterate through the text entries\n",
    "    for entry in text_entries:\n",
    "        # Flag to check if the entry is a duplicate\n",
    "        is_duplicate = False\n",
    "\n",
    "        # Compare the entry with each deduplicated entry\n",
    "        for dedup_entry in deduplicated_entries:\n",
    "            similarity = difflib.SequenceMatcher(None, entry, dedup_entry).ratio()\n",
    "            if similarity >= similarity_threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "\n",
    "        # If the entry is not a duplicate, add it to the deduplicated list\n",
    "        if not is_duplicate:\n",
    "            deduplicated_entries.append(entry)\n",
    "\n",
    "    return deduplicated_entries\n",
    "\n",
    "# Specify a similarity threshold (adjust as needed)\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Remove redundancy\n",
    "deduplicated_text_entries = remove_redundancy(text_entries, similarity_threshold)\n",
    "\n",
    "# Print the deduplicated entries\n",
    "for entry in deduplicated_text_entries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3JGeZPOfZ94T"
   },
   "outputs": [],
   "source": [
    "text = \"Cats and dogs are friends.\"\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zw6_2y2cZ90_"
   },
   "outputs": [],
   "source": [
    "vocab = set(tokens)\n",
    "one_hot_encoded = []\n",
    "for token in tokens:\n",
    "    one_hot_vector = [1 if token == word else 0 for word in vocab]\n",
    "    one_hot_encoded.append(one_hot_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODNIVXO0Z9yG",
    "outputId": "2a1d229b-b1f8-4a02-eb17-c268cb2667aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX3Z5VZCZ9vx",
    "outputId": "f4c4d477-1555-4300-d0f4-5bfb9eede902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "flattened_vector = [item for sublist in one_hot_encoded for item in sublist]\n",
    "print(flattened_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXj5itMHZ9tQ",
    "outputId": "b8140fc0-e3b8-4838-99fb-7207eeac5672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking: 1\n",
      "down: 1\n",
      "individual: 1\n",
      "into: 1\n",
      "is: 1\n",
      "of: 1\n",
      "or: 1\n",
      "process: 1\n",
      "text: 1\n",
      "the: 1\n",
      "tokenization: 1\n",
      "tokens: 1\n",
      "words: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit the vectorizer on the text and transform the text into a one-hot encoded matrix\n",
    "one_hot_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the one-hot matrix to a dense array\n",
    "one_hot_array = one_hot_matrix.toarray()\n",
    "\n",
    "# Create a dictionary to map words to their one-hot encoding\n",
    "one_hot_encoding = {word: one_hot_array[0][i] for i, word in enumerate(feature_names)}\n",
    "\n",
    "# Print the one-hot encoding for each word\n",
    "for word, encoding in one_hot_encoding.items():\n",
    "    print(f\"{word}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "re1cartUZ9rI",
    "outputId": "b6d43cb5-a631-4747-b23c-bb38612416d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking: [1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"Tokenization is the process of breaking down a text into individual words or tokens.\"\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit the vectorizer on the text and transform the text into a one-hot encoded matrix\n",
    "one_hot_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the one-hot matrix to a dense array\n",
    "one_hot_vectors = one_hot_matrix.toarray()\n",
    "\n",
    "# Create a dictionary to map words to their one-hot encoding\n",
    "one_hot_encoding = {word: vector for word, vector in zip(feature_names, one_hot_vectors)}\n",
    "\n",
    "# Print the one-hot encoding vectors for each word\n",
    "for word, encoding_vector in one_hot_encoding.items():\n",
    "    print(f\"{word}: {encoding_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O5DHpMSH4a-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
